{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkKPn7Z0pH49"
      },
      "source": [
        "Retrieval-Augmented Generation with LlamaIndex ü¶ôüìö\n",
        "\n",
        "> **Learning Objectives:**\n",
        "> * üß† Understand the principles of Retrieval-Augmented Generation (RAG) and LlamaIndex\n",
        "> * üõ†Ô∏è Set up a development environment for LlamaIndex\n",
        "> * üîç Build and customize a basic RAG pipeline\n",
        "> * üìä Explore different index types in LlamaIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qj_mVnMpLKp"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "\n",
        "LlamaIndex is a data framework that simplifies the process of connecting custom data sources to LLMs. It provides tools for data ingestion, structuring, and efficient retrieval, making it easier to build robust RAG systems.\n",
        "\n",
        "\n",
        "> üí° **Retrieval-Augmented Generation (RAG)** is a technique that enhances large language models (LLMs) with external knowledge.\n",
        ">\n",
        "> Instead of relying solely on the model's pre-trained knowledge, RAG systems retrieve relevant information from a knowledge base to generate more accurate and contextually appropriate responses.\n",
        "\n",
        "\n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=19zHBeEQrvtUgRy7ZdALmn1AASNq_uC5F'>\n",
        "\n",
        "[Source](https://arxiv.org/abs/2402.19473)\n",
        "\n",
        "\n",
        "\n",
        "ü¶ô **LlamaIndex** is a data framework that simplifies the process of connecting custom data sources to LLMs. It provides tools for data ingestion, structuring, and efficient retrieval, making it easier to build robust RAG systems.\n",
        "\n",
        "üîë Key Concepts:\n",
        "- **RAG**: A technique that combines retrieval of relevant information with generative AI to produce more accurate and contextual responses.\n",
        "- **Documents**: Raw text data, like files or web pages, that serve as the knowledge source.\n",
        "- **Nodes**: Smaller, more manageable pieces of information extracted from documents.\n",
        "- **Indices**: Data structures that organize nodes for efficient searching and retrieval.\n",
        "- **Query Engine**: The component that processes user queries and generates responses using the index and LLM.\n",
        "\n",
        "> Check out the LlamaIndex [documentation](https://docs.llamaindex.ai/en/stable/) for more information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjtTxkL36iOq"
      },
      "source": [
        "\n",
        "\n",
        "Let's get started by setting up our environment!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qrvo51YRq4VV"
      },
      "source": [
        "### Environment Setup\n",
        "\n",
        "First, we need to install the necessary libraries and set up our OpenAI API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "C-_rcvmoq6Qx",
        "outputId": "9ded6f60-de7d-42fd-8c53-0a46104feb75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m173.8/173.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m309.6/309.6 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m208.9/208.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install llama-index --quiet\n",
        "!pip install openai --quiet\n",
        "!pip install python-dotenv --quiet\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLvH2cpvq_EC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Set OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
        "# Initialize the model\n",
        "\n",
        "# Create a service context\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "print(\"Environment set up successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCZqBFY8rRvy"
      },
      "source": [
        "## Exercise 1 - Building a Basic RAG Pipeline\n",
        "\n",
        "Now that our environment is set up, let's create a simple RAG system using LlamaIndex. We'll start by creating some sample documents and then build an index to query them.\n",
        "\n",
        "üß† Understanding the Pipeline:\n",
        "\n",
        "1. We create sample documents to serve as our knowledge base.\n",
        "\n",
        "2. The `SimpleDirectoryReader` loads these documents into our system.\n",
        "\n",
        "3. We create a `VectorStoreIndex`, which converts our documents into vector embeddings for efficient semantic search.\n",
        "\n",
        "4. The `as_query_engine()` method creates an interface for querying our index.\n",
        "\n",
        "5. Finally, we test our system with a simple query.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgRYJgP-w1Z7"
      },
      "source": [
        "### 1.2 Loading Sample Documents\n",
        "\n",
        "Let's first create some sample documents for this lab, before proceeding to querying them. Go ahead and run the cell below to proceed storing our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6IVAdi0wz5h"
      },
      "outputs": [],
      "source": [
        "# Sample Document 1: Introduction to Machine Learning\n",
        "ml_doc = \"\"\"\n",
        "Machine Learning is a subset of artificial intelligence that focuses on the development of algorithms and statistical models that enable computer systems to improve their performance on a specific task through experience. Unlike traditional programming, where explicit instructions are provided to solve a problem, machine learning algorithms use data to learn patterns and make predictions or decisions without being explicitly programmed.\n",
        "\n",
        "Key concepts in machine learning include:\n",
        "\n",
        "1. Supervised Learning: The algorithm learns from labeled training data, attempting to find a function that best maps input variables to output variables.\n",
        "\n",
        "2. Unsupervised Learning: The algorithm works on unlabeled data, trying to find hidden structures or patterns within the data.\n",
        "\n",
        "3. Reinforcement Learning: The algorithm learns to make decisions by performing actions in an environment to maximize a reward signal.\n",
        "\n",
        "4. Deep Learning: A subset of machine learning based on artificial neural networks with multiple layers, capable of learning complex patterns in large amounts of data.\n",
        "\n",
        "Machine learning has numerous applications across various fields, including:\n",
        "- Image and speech recognition\n",
        "- Natural language processing\n",
        "- Recommendation systems\n",
        "- Autonomous vehicles\n",
        "- Medical diagnosis\n",
        "- Financial forecasting\n",
        "\n",
        "As the field continues to evolve, new techniques and applications are constantly emerging, making machine learning one of the most exciting and rapidly growing areas in computer science and artificial intelligence.\n",
        "\"\"\"\n",
        "\n",
        "# Sample Document 2: The Impact of Climate Change\n",
        "climate_doc = \"\"\"\n",
        "Climate change refers to long-term shifts in global weather patterns and average temperatures. Primarily caused by human activities, particularly the burning of fossil fuels, climate change is one of the most pressing issues facing our planet today. The impacts of climate change are far-reaching and affect every aspect of life on Earth.\n",
        "\n",
        "Key effects of climate change include:\n",
        "\n",
        "1. Rising Global Temperatures: The Earth's average temperature has increased by about 1¬∞C since pre-industrial times, with most of this warming occurring in the past 40 years.\n",
        "\n",
        "2. Extreme Weather Events: Climate change is leading to more frequent and severe heatwaves, droughts, hurricanes, and floods.\n",
        "\n",
        "3. Sea Level Rise: Melting ice caps and glaciers, combined with thermal expansion of the oceans, are causing sea levels to rise, threatening coastal communities and ecosystems.\n",
        "\n",
        "4. Biodiversity Loss: Changing temperatures and weather patterns are forcing many species to adapt or migrate, with some facing extinction.\n",
        "\n",
        "5. Food and Water Security: Altered precipitation patterns and rising temperatures affect agriculture and water availability, potentially leading to food and water shortages.\n",
        "\n",
        "6. Human Health: Climate change impacts human health through increased air pollution, the spread of infectious diseases, and heat-related illnesses.\n",
        "\n",
        "Addressing climate change requires a multi-faceted approach, including:\n",
        "- Transitioning to renewable energy sources\n",
        "- Improving energy efficiency\n",
        "- Sustainable land use and forest management\n",
        "- Developing climate-resilient infrastructure\n",
        "- International cooperation and policy implementation\n",
        "\n",
        "The urgency of the climate crisis calls for immediate and decisive action from governments, businesses, and individuals worldwide to mitigate its effects and adapt to the changes already underway.\n",
        "\"\"\"\n",
        "\n",
        "# Sample Document 3: The Evolution of Artificial Intelligence\n",
        "ai_doc = \"\"\"\n",
        "Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines capable of mimicking human cognitive functions such as learning, problem-solving, and decision-making. The field has evolved significantly since its inception in the 1950s, with major breakthroughs and paradigm shifts shaping its development.\n",
        "\n",
        "Key milestones in the evolution of AI include:\n",
        "\n",
        "1. Early AI (1950s-1970s): Focused on symbolic AI and rule-based systems. This era saw the development of the Turing Test, the first AI programs like the Logic Theorist, and expert systems.\n",
        "\n",
        "2. AI Winter (1970s-1980s): A period of reduced funding and interest in AI due to overhyped promises and limited progress.\n",
        "\n",
        "3. Machine Learning Revolution (1990s-2000s): The rise of statistical approaches and machine learning algorithms, including support vector machines and decision trees.\n",
        "\n",
        "4. Deep Learning Breakthrough (2010s-present): Advances in neural networks and deep learning, fueled by increased computational power and big data, led to significant progress in areas like computer vision and natural language processing.\n",
        "\n",
        "5. Current Trends:\n",
        "   - Reinforcement Learning: AI systems learning through interaction with environments.\n",
        "   - Generative AI: Models capable of creating new content, including text, images, and audio.\n",
        "   - Explainable AI: Developing methods to make AI decision-making processes more transparent and interpretable.\n",
        "   - AI Ethics: Addressing concerns about bias, privacy, and the societal impact of AI.\n",
        "\n",
        "Applications of modern AI span various domains:\n",
        "- Healthcare: Disease diagnosis, drug discovery, and personalized treatment plans.\n",
        "- Finance: Algorithmic trading, fraud detection, and risk assessment.\n",
        "- Transportation: Autonomous vehicles and traffic management systems.\n",
        "- Entertainment: Personalized content recommendations and AI-generated art.\n",
        "- Education: Adaptive learning systems and automated grading.\n",
        "\n",
        "As AI continues to advance, it raises important questions about the future of work, privacy, and the relationship between humans and machines. The field's ongoing development promises to revolutionize numerous aspects of society while also presenting new challenges and ethical considerations.\n",
        "\"\"\"\n",
        "!mkdir docs\n",
        "# Write the documents to files\n",
        "with open(\"./docs/machine_learning.txt\", \"w\") as f:\n",
        "    f.write(ml_doc)\n",
        "\n",
        "with open(\"./docs/climate_change.txt\", \"w\") as f:\n",
        "    f.write(climate_doc)\n",
        "\n",
        "with open(\"./docs/artificial_intelligence.txt\", \"w\") as f:\n",
        "    f.write(ai_doc)\n",
        "\n",
        "print(\"Sample documents created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbSEw3V1wzAX"
      },
      "source": [
        "### 1.2 Creating RAG Pipeline\n",
        "\n",
        "Now we'll use llama index to create an index and ask information about the stored documents. You are encouraged to review the documents yourself to ensure quality of our RAG's results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_Rf-x63ro_8"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "# Load documents\n",
        "documents = SimpleDirectoryReader('./docs/').load_data()\n",
        "print(f\"Loaded {len(documents)} documents\")\n",
        "\n",
        "# Create an index\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Create a query engine\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Test the query engine\n",
        "response = query_engine.query(\"Which documents do we have loaded? Give brief bullet descriptions for each\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwQbcVPn5nnZ"
      },
      "source": [
        "### üí° Why This Matters:\n",
        "> You've just created an AI system that can understand and answer questions about custom data. Consider how this could be applied in various fields such as customer service, research assistance, or knowledge management.\n",
        ">\n",
        "> You may have also noticed that it is quite easy to prototype RAGs quickly with LllamaIndex with just a few short lines of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqEUXLPBr4Tp"
      },
      "source": [
        "### üß™ Experiment\n",
        "\n",
        "\n",
        "1. Try querying your RAG system with different questions related to your chosen topic.\n",
        "\n",
        "2. How does the system handle questions about information not present in your data? Test this and explain what you observe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzofo0nRr5kb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Querying with different questions\n",
        "questions = [\n",
        "    \"What are the main benefits of Artifical Intelligence?\",\n",
        "    \"How is Machine learning related to AI?\",\n",
        "    \"What should citizens know about Climate Change?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    response = query_engine.query(question)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {response}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KycH3HYuyaH4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 2. Out of scope questions\n",
        "out_of_scope_questions = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"How do I bake a chocolate cake?\",\n",
        "    \"Who won the World Cup in 2022?\"\n",
        "]\n",
        "\n",
        "for question in out_of_scope_questions:\n",
        "    response = query_engine.query(question)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {response}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1RVQRdy6zKS"
      },
      "source": [
        "## Exercise 2 - Exploring Index Types\n",
        "\n",
        "LlamaIndex offers various index types, each suited for different use cases. Let's explore three common types: Vector Store Index, Summary Index, and Tree Index.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5vRN_1wr-qG"
      },
      "source": [
        "\n",
        "### 2.1 Vector Store Index\n",
        "\n",
        "We've already used this in our basic pipeline. It's great for semantic search and retrieving contextually relevant information.\n",
        "\n",
        "\n",
        "\n",
        "üß† The Vector Store Index creates embeddings for each chunk of text, allowing for semantic similarity search. This is particularly useful when you need to find information based on meaning rather than exact keyword matches.\n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=1OqZf4eet5RBjOrKydyR6ChnTygPAziFh'>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBPUMUuj2tVF"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "vector_index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
        "query_engine = vector_index.as_query_engine()\n",
        "\n",
        "response = query_engine.query(\"Explain like I'm 5: What are ML and AI?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sRJMJjK31Py"
      },
      "source": [
        "### 2.2 Summary Index\n",
        "\n",
        "The Summary Index is simpler and focuses on generating summaries of the ingested documents.\n",
        "\n",
        "üß† The Summary Index is ideal when you need high-level overviews of your data. It doesn't perform semantic search but instead focuses on condensing information.\n",
        "<img src = 'https://drive.google.com/uc?id=1IQbRTxq9wBVfqy3lVfgxVJToWSivaThW'>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5ZktVux36bp"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SummaryIndex\n",
        "\n",
        "summary_index = SummaryIndex.from_documents(documents)\n",
        "query_engine = summary_index.as_query_engine()\n",
        "\n",
        "response = query_engine.query(\"Summarize the key points from all documents with bullets\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T48UUyo73L35"
      },
      "source": [
        "### 2.3 Tree Index\n",
        "\n",
        "This index type is used for datasets that contain hierarchical data.\n",
        "\n",
        "\n",
        "üß† The Tree Index is powerful for understanding relationships between different pieces of information of a hierarchical nature. It's particularly useful in domains where the hierarchical relationships between concepts are essential to understanding the data.\n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=1LmxVOPEG6neEjwHZ9QGapCzdkPWMuyIT' width = 500>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpfcnpOE2aqC"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import TreeIndex\n",
        "\n",
        "tree_index = TreeIndex.from_documents(documents)\n",
        "query_engine = tree_index.as_query_engine()\n",
        "\n",
        "response = query_engine.query(\"What are the relationships between ML and AI?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GTOkrcj3SJO"
      },
      "source": [
        "**You can learn more about the details of Query Indexing strategies in LlamaIndex's website [here](https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide/)**\n",
        "\n",
        "### 2.4 Index Suitability\n",
        "\n",
        "1. **Vector Store Index**: Best for semantic search and finding contextually relevant information. Suitable for applications like chatbots or question-answering systems where understanding the context is crucial.\n",
        "\n",
        "2. **Summary Index**: Ideal for generating overviews or summaries of large documents. Useful in applications that need to provide quick, high-level insights from extensive data.\n",
        "\n",
        "3. **Tree Index**: Excellent for understanding hierarchical information between concepts. Well-suited for complex domains where hierchical nature is relevant (e.g. Medical Domain - Medical Ontology data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga4sdkfVtyRB"
      },
      "source": [
        "\n",
        "### üß™ Experiment\n",
        "\n",
        "\n",
        "1. Build each type of index (Vector Store, Summary, and Knowledge Graph) using these documents.\n",
        "2. Query each index with the same question and compare the results. How do the responses differ?\n",
        "3. Reflect on which index type might be most suitable for different types of applications or datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H6cnCJ_tzpI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SummaryIndex, KnowledgeGraphIndex\n",
        "\n",
        "# Vector Store Index\n",
        "vector_index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Summary Index\n",
        "summary_index = SummaryIndex.from_documents(documents)\n",
        "\n",
        "# Knowledge Graph Index\n",
        "kg_index = KnowledgeGraphIndex.from_documents(documents)\n",
        "\n",
        "# Create query engines for each index\n",
        "vector_query_engine = vector_index.as_query_engine()\n",
        "summary_query_engine = summary_index.as_query_engine()\n",
        "kg_query_engine = kg_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w45k2s27GWz"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "question = \"What are the key challenges in artificial intelligence and machine learning?\"\n",
        "\n",
        "print(\"Vector Store Index Response:\")\n",
        "print(vector_query_engine.query(question))\n",
        "\n",
        "print(\"\\nSummary Index Response:\")\n",
        "print(summary_query_engine.query(question))\n",
        "\n",
        "print(\"\\nKnowledge Graph Index Response:\")\n",
        "print(kg_query_engine.query(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amZx2fd8ERDC"
      },
      "source": [
        "\n",
        "## Exercise 3 - Storing Your Index\n",
        "\n",
        "Now that we've explored different index types and customization options, let's learn how to store our indexed data for future use. This is crucial for avoiding the time and cost associated with re-indexing large datasets.\n",
        "\n",
        "üß† **Why Store Indices?**\n",
        "\n",
        "Storing indices allows you to:\n",
        "1. Save time by avoiding repeated data processing and embedding generation\n",
        "2. Reduce costs, especially when working with large datasets or expensive embedding models\n",
        "3. Maintain consistency across different runs or application instances\n",
        "4. Quickly load and use your indexed data in production environments\n",
        "\n",
        "Let's explore two main methods of storing our index: persisting to disk and using vector stores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAtURB9dEXef"
      },
      "source": [
        "\n",
        "### 3.1 Persisting to Disk\n",
        "\n",
        "The simplest way to store your indexed data is by using the built-in `.persist()` method. This works for any type of index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF7UjA0_EZ7k"
      },
      "outputs": [],
      "source": [
        "# Assuming we're using the vector_index we created earlier\n",
        "persist_dir = \"./stored_index\"\n",
        "vector_index.storage_context.persist(persist_dir=persist_dir)\n",
        "\n",
        "print(f\"Index persisted to {persist_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTW4gogqEbuY"
      },
      "source": [
        "To load the persisted index later:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ3UdExpEcsj"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "\n",
        "# Rebuild storage context\n",
        "storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
        "\n",
        "# Load index\n",
        "loaded_index = load_index_from_storage(storage_context)\n",
        "\n",
        "print(\"Index loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1vhc46lEfgn"
      },
      "source": [
        "\n",
        "### 3.2 Using Vector Stores\n",
        "\n",
        "For more advanced storage and faster retrieval, especially with large datasets, we can use specialized vector stores. Let's use Chroma as an example.\n",
        "\n",
        "First, install Chroma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlSQOd-CDbJU"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb --quiet\n",
        "!pip install llama_index.vector_stores.chroma --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHcJ83lAEqMW"
      },
      "source": [
        "Now, let's set up Chroma and store our index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doc76XOUDbGR"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "# Initialize Chroma client\n",
        "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# Create or get a collection\n",
        "chroma_collection = db.get_or_create_collection(\"llamaindex_lab\")\n",
        "\n",
        "# Create ChromaVectorStore\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# Create a new storage context using the vector store\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Create a new index using the storage context\n",
        "chroma_index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context\n",
        ")\n",
        "\n",
        "print(\"Index stored in Chroma successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiwkIpH2EuIg"
      },
      "source": [
        "To load and use the Chroma-stored index later:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op_kEtHqDbDR"
      },
      "outputs": [],
      "source": [
        "# Get the existing collection\n",
        "chroma_collection = db.get_collection(\"llamaindex_lab\")\n",
        "\n",
        "# Create ChromaVectorStore\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# Load the index\n",
        "loaded_chroma_index = VectorStoreIndex.from_vector_store(vector_store)\n",
        "\n",
        "print(\"Chroma-stored index loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTTkVJNXExnO"
      },
      "source": [
        "\n",
        "### üß™ Experiment\n",
        "\n",
        "1. Experiment with persisting different types of indices (e.g., SummaryIndex, TreeIndex) to disk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hiHEU8nE2P2"
      },
      "outputs": [],
      "source": [
        "\n",
        "from llama_index.core import SummaryIndex, TreeIndex\n",
        "\n",
        "# Create and persist SummaryIndex\n",
        "summary_index = SummaryIndex.from_documents(documents)\n",
        "summary_index.storage_context.persist(persist_dir=\"./summary_index\")\n",
        "\n",
        "# Create and persist TreeIndex\n",
        "kg_index = TreeIndex.from_documents(documents)\n",
        "kg_index.storage_context.persist(persist_dir=\"./kg_index\")\n",
        "\n",
        "print(\"Indices persisted successfully!\")\n",
        "\n",
        "# Loading persisted indices\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "\n",
        "loaded_summary_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./summary_index\"))\n",
        "loaded_kg_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./kg_index\"))\n",
        "\n",
        "print(\"Indices loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiWsGm-WE5rO"
      },
      "source": [
        "## Exercise 4 - Advanced Querying Techniques\n",
        "\n",
        "Now that we have our index stored, let's explore some advanced querying techniques to get the most out of our RAG system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4GqDe5WE9sl"
      },
      "source": [
        "### 4.1 Customizing the Query Pipeline\n",
        "\n",
        "LlamaIndex allows you to customize various stages of the querying process. Let's break down the querying pipeline and customize each part:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcTOBQRzFARQ"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import get_response_synthesizer\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "\n",
        "# Custom retriever\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=loaded_chroma_index,\n",
        "    similarity_top_k=5  # Retrieve top 5 most similar nodes\n",
        ")\n",
        "\n",
        "# Custom response synthesizer\n",
        "response_synthesizer = get_response_synthesizer(\n",
        "    response_mode=\"compact\"  # Other options: \"default\", \"tree_summarize\", \"accumulate\"\n",
        ")\n",
        "\n",
        "# Custom post-processor\n",
        "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)\n",
        "\n",
        "# Assemble custom query engine\n",
        "custom_query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        "    node_postprocessors=[postprocessor]\n",
        ")\n",
        "\n",
        "# Query using the custom engine\n",
        "response = custom_query_engine.query(\"What are all the main applications of AI in healthcare?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71j9L2yMFHGv"
      },
      "source": [
        "### 4.2 Structured Outputs\n",
        "\n",
        "Sometimes, you may want to ensure that your RAG system's outputs follow a specific structure. We can use Pydantic models to achieve this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD7q52i3FQIy"
      },
      "outputs": [],
      "source": [
        "!pip install llama_index.core --quiet\n",
        "!pip install pydantic --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sc5zavOGmdX4"
      },
      "outputs": [],
      "source": [
        "from llama_index.program.openai import OpenAIPydanticProgram\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "prompt_template_str = \"\"\"\\\n",
        "Generate 4 applications of Artificial Intelligence in Healthcare.\n",
        "\"\"\"\n",
        "\n",
        "class Application(BaseModel):\n",
        "    \"\"\"Data model for a Application.\"\"\"\n",
        "\n",
        "    application: str = Field(description=\"Name of the application\")\n",
        "    description: str = Field(description=\"Brief Description of the application\")\n",
        "    benefits: str = Field(description=\"Key benefits of using this application\")\n",
        "    use_cases: str = Field(description=\"Examples of use cases for this application\")\n",
        "\n",
        "\n",
        "class AIApplications(BaseModel):\n",
        "    \"\"\"Data model for an Application.\"\"\"\n",
        "\n",
        "\n",
        "    applications: List[Application]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQfF74e4oWgC"
      },
      "outputs": [],
      "source": [
        "prompt_template_str = \"\"\"\\\n",
        "Generate 4 applications of Artificial Intelligence in Healthcare.\n",
        "\"\"\"\n",
        "program = OpenAIPydanticProgram.from_defaults(\n",
        "    output_cls=AIApplications,\n",
        "    prompt_template_str=prompt_template_str,\n",
        "    verbose=True,\n",
        "    index = index\n",
        ")\n",
        "output = program()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au6_g6dIpe4c"
      },
      "source": [
        "\n",
        "\n",
        "### üß™ Experiment\n",
        "\n",
        "1. Build a Structured output class for understanding key issues in Climate change.\n",
        "\n",
        "2. Test your class by making a `program` using `OpenAIPydanticProgram` and return the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC-C-VnvohV9"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "# 1. Build Structured output class\n",
        "from llama_index.program.openai import OpenAIPydanticProgram\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "\n",
        "class ClimateChange(BaseModel):\n",
        "    \"\"\"Data model for a Climate change Issue.\"\"\"\n",
        "\n",
        "    issue: str = Field(description=\"Name of the climate change issue\")\n",
        "    description: str = Field(description=\"Brief Description of the climate change issue\")\n",
        "    action: str = Field(description=\"Action to be taken to address the issue\")\n",
        "    impact: str = Field(description=\"Impact of the issue\")\n",
        "    mitigation: str = Field(description=\"Mitigation measures to address the issue\")\n",
        "\n",
        "class ClimateIssues(BaseModel):\n",
        "    \"\"\"Data model for list of Climate change issues.\"\"\"\n",
        "\n",
        "    issues: List[ClimateChange]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Res7PBZ6qQrG"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "# 2. Test our Climate change class  `program`\n",
        "prompt_template_str = \"\"\"\\\n",
        "Generate 4 key issues in Climate change.\n",
        "\"\"\"\n",
        "program = OpenAIPydanticProgram.from_defaults(\n",
        "    output_cls=ClimateIssues,\n",
        "    prompt_template_str=prompt_template_str,\n",
        "    verbose=True,\n",
        "    index = index\n",
        ")\n",
        "output = program()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s39yKCv3vC4F"
      },
      "source": [
        "### Conclusion üéì\n",
        "\n",
        "In this extended lab, we've explored advanced techniques for storing indices and performing complex queries using LlamaIndex. We've learned how to:\n",
        "\n",
        "- üóÇÔ∏è **Persist indices to disk and use vector stores for efficient storage and retrieval**\n",
        "- üîß **Customize the query pipeline for more precise and relevant responses**\n",
        "- üìã **Generate structured outputs using Pydantic models**\n",
        "- üîç **Perform multi-step querying for complex questions**\n",
        "\n",
        "These techniques allow you to build more sophisticated and powerful RAG systems, capable of handling a wide range of use cases and query complexities.\n",
        "\n",
        "üîë Remember that the key to building effective RAG systems is experimentation and iteration. Don't hesitate to try different combinations of storage methods, query techniques, and output structures to find what works best for your specific use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCpMLhmkvFll"
      },
      "source": [
        "## üöÄ Additional Exercises\n",
        "\n",
        "1. Implement a caching mechanism to store and reuse query results for frequently asked questions.\n",
        "\n",
        "2. Create a query pipeline that combines multiple index types (e.g., vector store and knowledge graph) for more comprehensive responses.\n",
        "\n",
        "3. Develop a simple API endpoint that exposes your RAG system, allowing users to query it programmatically.\n",
        "\n",
        "4. Experiment with different LLMs (e.g., GPT-4, Claude) and compare their performance in your RAG system.\n",
        "\n",
        "5. Implement a feedback loop that allows users to rate responses and use this feedback to improve the retrieval and synthesis processes over time."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "conda_pytorch_p310",
      "language": "python",
      "name": "conda_pytorch_p310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}